# Technical Test ‚Äì Data Pipeline (Postgres + Docker) & Python (Secci√≥n 2)

## Stack

* **Docker / Docker Compose**
* **PostgreSQL 16**
* **pgAdmin 4**
* **Python 3.11** (en contenedor `etl`) con: `pandas`, `sqlalchemy[psycopg]`, `pyarrow` (opcional)

## Prerrequisitos

* macOS / Windows / Linux con **Docker Desktop** instalado.
* (Opcional) **VS Code**.

## Estructura del proyecto

```
technical-test/
‚îú‚îÄ docker-compose.yml
‚îú‚îÄ .env                        # credenciales locales 
‚îú‚îÄ datasets/
‚îÇ  ‚îú‚îÄ input.csv                # dataset de entrada (copiar aqu√≠)
‚îÇ  ‚îî‚îÄ extracted.csv            # generado por extracci√≥n (1.2)
‚îú‚îÄ etl/
‚îÇ  ‚îú‚îÄ Dockerfile
‚îÇ  ‚îú‚îÄ requirements.txt
‚îÇ  ‚îî‚îÄ src/
‚îÇ     ‚îú‚îÄ load_raw.py           # 1.1
‚îÇ     ‚îú‚îÄ extract.py            # 1.2
‚îÇ     ‚îî‚îÄ transform.py          # 1.3 + 1.4
‚îî‚îÄ sql/
   ‚îî‚îÄ (opcional, DDL/diagramas)
```

---

## 0) Configuraci√≥n r√°pida

1. **Clonar y entrar al proyecto:**

```bash
git clone https://github.com/duranueva/technical-test.git
cd technical-test
```

2. **Crear archivo `.env`**

### MacOS / Linux

```bash
cat > .env << 'EOF'
PGUSER=postgres
PGPASSWORD=postgres
PGDATABASE=warehouse
PGHOST=db
PGPORT=5432

PGADMIN_DEFAULT_EMAIL=admin@example.com
PGADMIN_DEFAULT_PASSWORD=admin
EOF
```

### Windows (PowerShell)

```powershell
@'
PGUSER=postgres
PGPASSWORD=postgres
PGDATABASE=warehouse
PGHOST=db
PGPORT=5432

PGADMIN_DEFAULT_EMAIL=admin@example.com
PGADMIN_DEFAULT_PASSWORD=admin
'@ | Out-File -FilePath .env -Encoding utf8
```


---

## 1) Levantar servicios (Postgres + pgAdmin)

```bash
docker compose up -d db pgadmin
```

* Postgres en `localhost:5432`
* pgAdmin en `http://localhost:8080` (login: `admin@example.com` / `admin`)



---

## 2) Construir la imagen del ETL

```bash
docker compose build etl
```

---

## 3) Secci√≥n 1 ‚Äî Data Pipeline

### 1.1 Carga RAW a Postgres

Carga el CSV **en crudo** a `raw.raw_purchases`:

```bash
docker compose run --rm etl python src/load_raw.py --input datasets/input.csv --table raw_purchases --schema raw --if-exists replace
```

Verificar:

```bash
docker compose exec db psql -U postgres -d warehouse -c "SELECT COUNT(*) FROM raw.raw_purchases;"
```

### 1.2 Extracci√≥n (CSV)

Exporta a CSV desde RAW (genera datasets/extracted.csv):

```bash
docker compose run --rm etl python src/extract.py
```

### 1.3 + 1.4 Transformaci√≥n y Dispersi√≥n

Transforma al esquema objetivo y carga a tablas finales **companies** y **charges** con FK:

```bash
docker compose run --rm etl  python src/transform.py  --input datasets/extracted.csv  --if-exists replace
```

Verificar:

```bash
docker compose exec db psql -U postgres -d warehouse -c "SELECT COUNT(*) FROM companies;"
```

```bash
docker compose exec db psql -U postgres -d warehouse -c "SELECT COUNT(*) FROM charges;"
```

**El diagrama de las tablas creadas.**

<p align="center">
  <img src="/img/mr.png" alt="Description" width="1000">
</p> 


### 1.5 Vista: total transaccionado por d√≠a y compa√±√≠a

```bash
docker compose exec db psql -U postgres -d warehouse -c "
CREATE OR REPLACE VIEW v_daily_totals AS
SELECT
  DATE(c.created_at) AS day,
  comp.company_name,
  comp.id AS company_id,
  SUM(c.amount) AS total_amount
FROM charges c
JOIN companies comp 
  ON comp.id = c.company_id
GROUP BY DATE(c.created_at), comp.company_name, comp.id;
"
docker compose exec db psql -U postgres -d warehouse -c "SELECT * FROM v_daily_totals ORDER BY day DESC LIMIT 10;"
```

üîπ **Copiar y ejecutar seg√∫n tu sistema operativo**:

* **Mac/Linux (bash/zsh):** puedes copiar exactamente el bloque de arriba.
* **Windows (PowerShell):** copia y pega los siguientes:

```powershell
docker compose exec db psql -U postgres -d warehouse -c "CREATE OR REPLACE VIEW v_daily_totals AS SELECT DATE(c.created_at) AS day, comp.company_name, comp.id AS company_id, SUM(c.amount) AS total_amount FROM charges c JOIN companies comp ON comp.id = c.company_id GROUP BY 1,2,3;"
```
```powershell
docker compose exec db psql -U postgres -d warehouse -c "SELECT * FROM v_daily_totals ORDER BY day DESC LIMIT 10;"
```

> **Nota**: si `psql` abre un ‚Äúpager‚Äù, sal con la tecla **`q`**.





---

## 4) Secci√≥n 2 ‚Äî Python (n√∫mero faltante 1..100)


```bash
docker compose run --rm etl python src/missing_number.py --extract 37
```

*(El CLI acepta `--extract N` para simular la extracci√≥n y luego calcula el faltante.)*

---

## 5) pgAdmin (opcional)

1. Abrir `http://localhost:8080`, login con `.env`.
2. Add New Server ‚Üí **Host**: `db`, **Port**: `5432`, **User**: `postgres`, **Password**: `postgres`.
3. Explora esquemas/tablas (`raw.raw_purchases`, `public.companies`, `public.charges`, vista `v_daily_totals`).

---


## 6) Limpieza

```bash
docker compose down -v
```

---

## Observaciones importantes y Analisis

### Secci√≥n 1
* **ETL robusto y repetible**  
  El ETL est√° preparado para ejecutarse m√∫ltiples veces sin duplicar datos finales. Para manejar tipos, el `.csv` se carg√≥ primero en **pandas** y de ah√≠ a Postgres, lo que facilita inspeccionar y tipar correctamente durante el ingreso. Hubo complicaciones con la columna **`amount`**: algunos valores eran muy grandes, as√≠ que en **RAW** se guardaron como **String** y se procesaron como n√∫mero en **Transformaci√≥n**, con validaciones y l√≠mites antes de cargar en **`charges.amount DECIMAL(16,2)`**.  

* **Lenguaje: Python**  
  Eleg√≠ **Python** porque tiene librer√≠as muy s√≥lidas para el tratamiento de datos (**pandas, sqlalchemy**) y porque es un lenguaje r√°pido de escribir, ampliamente usado en ciencia de datos e ingenier√≠a de datos. Esto permiti√≥ implementar el pipeline de extracci√≥n y transformaci√≥n de manera clara y legible.  

* **Formato: CSV**  
  Eleg√≠ mantener la extracci√≥n final en **CSV** porque el dataset era relativamente peque√±o y tabular, lo que hace que CSV sea suficiente y universalmente compatible. Aunque formatos como Parquet hubieran sido m√°s eficientes para grandes vol√∫menes, aqu√≠ primaba la **simplicidad** y facilidad de inspecci√≥n de resultados.  

* **Retos durante la extracci√≥n**  
  Durante la fase de extracci√≥n, uno de los principales retos fue la **homogeneidad de tipos de datos**:  
  - `amount` ten√≠a valores demasiado grandes para el tipo definido inicialmente en Postgres.  
  - Algunas fechas (`created_at`, `paid_at`) ven√≠an vac√≠as o con distintos formatos, lo que requiri√≥ parseo y validaci√≥n.  
  Estos casos se resolvieron cargando primero a pandas y luego normalizando antes de escribir en la base final.  

* **Transformaci√≥n de datos**  
  En esta fase se aplicaron reglas de limpieza adicionales:  
  - Se descartaron valores corruptos en columnas clave (`id`, `company_id`, `status`).  

* **Elecci√≥n por Postgres**  
  La elecci√≥n viene por diversos motivos: se trata una base de datos relacional madura, ampliamente usada en la industria, que garantiza integridad y consistencia de los datos, as√≠ como consultas anal√≠ticas complejas f√°cilmente, adem√°s de ser accesible para todos por ser open source, as√≠ como tambi√©n la simplicidad de utilizarla para los puntos 1.1 Carga de informacion y 1.4 Dispersi√≥n  



### Secci√≥n 2
* **Eficiencia en tiempo/espacio**  
  Evalu√© distintas opciones y us√© el enfoque con **XOR** para calcular el n√∫mero faltante en **O(1)** de espacio y **O(n)** de tiempo, lo que es √≥ptimo y simple. Incluyo a continuaci√≥n fuentes que consult√©.  



### Decisi√≥n por Stack
* Dado que el tiempo se consideraba libre, opt√© por utilizar herramientas m√°s robustas (que suelen usarse en un entorno de producci√≥n) como lo son PostgreSQL, pgAdmin y Docker. De ser por un tiempo m√°s limitado habr√≠a optado por Notebooks de Python (Google Colab) y SQLite como base de datos, que permiten hacer el trabajo para esta prueba, pero consideraba que pod√≠a mostrar m√°s mis habilidades con un stack m√°s amplio.  


---

## Fuentes

* [Find the missing number in an array: Optimal Approach 2](https://takeuforward.org/arrays/find-the-missing-number-in-an-array/)

---

## Notas finales

* El proyecto se puede ejecutar en cualquier SO con Docker.
* Las credenciales est√°n en `.env` para simplificar la reproducci√≥n local.
* Por simplicidad, el dataset fue renombrado y se encuentra como: `datasets/input.csv`.
